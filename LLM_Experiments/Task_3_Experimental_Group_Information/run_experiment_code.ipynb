{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Experimental Group Information\n",
    "\n",
    "The code in this notebook runs the prompts and papers through the GPT-4o model. Please note that this code is not optimized and there are some details that are not essential to running the code through GPT-4o.\n",
    "\n",
    "## General Setup \n",
    "\n",
    "In the setup section, we do the imports, load the API key, set up an LLM kill switch (because OpenAI costs money), load the prompt, and load the ground truth data.\n",
    "\n",
    "### Imports & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function was originally used for token counting, but OpenAI's models have changed tokenization (circa. 5/2024) so this function should be updated if you intend to use it! (It is included here for completeness.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token counting function (for checking LLM token usage):\n",
    "\n",
    "def num_tokens_from_string(string: str, model_name: str=\"gpt-4\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kill-Switch\n",
    "\n",
    "The `LLM_run` variable is set to test before making any API calls ($$$). It is defaulted to `False` to prevent charges from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_run = False    # When True, the LLM will be used\n",
    "\n",
    "if LLM_run:\n",
    "    print(\"LLM set to Run.\")\n",
    "else:\n",
    "    print(\"LLM not to be called.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load API Keys\n",
    "\n",
    "All of this was built with DotEnv files, but it should work with API keys saved in the environment with the right names (see below). We also use `ROOT_DIR`, an environment variable set to the folder containing this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenvfile = find_dotenv()\n",
    "load_dotenv(dotenvfile)      # Apparently no issues if null\n",
    "\n",
    "if dotenvfile == '':\n",
    "    print(\"No dotenv file. If OpenAI key set in environment this should still run.\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "ROOT_DIR       = os.getenv('ROOT_DIR')\n",
    "print(\"ROOT_DIR set to: \" + ROOT_DIR)\n",
    "\n",
    "if OPENAI_API_KEY == '' or OPENAI_API_KEY is None:\n",
    "    print(\"No OpenAI API key found in environment or dotenv file. See https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key.\")\n",
    "else:\n",
    "    print(\"Using OpenAI API key from environment or dotenv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()  # Check to make sure it is local\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Specifics\n",
    "\n",
    "The models used in this research are listed in the next cell and can be selected for use as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = {\"gpt4p\":\"gpt-4-0125-preview\", \n",
    "              \"gpt4\":\"gpt-4-0613\", \n",
    "              \"gpt35t\":\"gpt-3.5-turbo-0125\",\n",
    "              \"gpt4o\":\"gpt-4o-2024-08-06\"}\n",
    "\n",
    "model = model_list[\"gpt4o\"]\n",
    "\n",
    "print(\"Using this model: \" + model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Prompts\n",
    "\n",
    "#### Load the basic task prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"zero-shot-prompt-disease-labels.txt\", \"r\") as f:\n",
    "    base_prompt = f.read()\n",
    "\n",
    "print(base_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens_from_string(base_prompt)   # Under the old tokenization model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a System Prompt\n",
    "\n",
    "This should describe the persona of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"\n",
    "You are a helpful assistant who is expert in reading and using standard psychiatric\n",
    "terminology and in translating between different systems of terminology. You understand\n",
    "science and experimental design. You are careful, thorough, and brief in your responses.\n",
    "You carefully read the text presented and correctly extract from it the information you\n",
    "need to respond to the user. You format your responses as JSON using the guide provided\n",
    "in the prompt.\n",
    "\"\"\"\n",
    "\n",
    "# As above for readability, the next lines reduce the whitespace for the LLM\n",
    "\n",
    "system_prompt_text = system_prompt_text.replace(\"\\n\", \" \")\n",
    "system_prompt_text = system_prompt_text.strip()\n",
    "num_tokens_from_string(system_prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(system_prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Message List\n",
    "\n",
    "Messages to the API must be formatted as a message list. The next function combines the system prompt and the (eventual) user prompt, which itself is the combination of the text of the paper added to the end of the `base_prompt`. This function **starts** a message list, but does not update them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_message_list(user_prompt, system_prompt = system_prompt_text):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "# Test\n",
    "\n",
    "start_message_list(\"This is the user message placeholder.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads the ground truth data for this test set, however we use only the list of `pmid`'s here as these are the publications we are analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = pd.read_csv(\"test-cases-with-ground-truth.csv\")\n",
    "print(gt_df.shape)\n",
    "gt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_list = gt_df['pmid'].tolist()\n",
    "paper_list = [x + \"_partial.txt\" for x in paper_list]\n",
    "print(paper_list)\n",
    "print(len(paper_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_articles_folder = ROOT_DIR + \"/Library/Articles_Title_Thru_Methods\"\n",
    "print(input_articles_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Initial Pass through the Papers to See the Lengths\n",
    "\n",
    "Just a note to other researchers: when this research started LLM context length was much shorter (often 4096 or 8192 tokens) so we did a lot to shorten the inputs. As models have developed this has become much less of a concern (except for per-token costs).\n",
    "\n",
    "We expect that in the future the entire papers will be uploaded without issues.\n",
    "\n",
    "The following code loads each paper and shows the token length under the old tokenization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_lengths = dict()\n",
    "for paper in paper_list:\n",
    "    with open(input_articles_folder + \"/\" + paper, \"r\") as f:\n",
    "        textblock = f.read()\n",
    "        submit = base_prompt + textblock\n",
    "        num_tokens = num_tokens_from_string(submit)\n",
    "        paper_lengths[paper] = num_tokens\n",
    "\n",
    "dict(sorted(paper_lengths.items(), key=lambda item: item[1])) # Min 2523, Max 6504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total\n",
    "\n",
    "sum(paper_lengths.values())  # Approximately 125k tokens for prompts and papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of values for number of tokens\n",
    "\n",
    "max(paper_lengths.values()), sum(paper_lengths.values())/len(paper_lengths), min(paper_lengths.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Cases\n",
    "\n",
    "The following code goes through all of the papers in the papers list, submits each with the prompt to the LLM, then saves the raw API return (as JSON) and the LLM response (also as JSON).\n",
    "\n",
    "We will use the LLM responses for analysis; the raw API return is logged for reference (model ID, fingerprint), but will likely not be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for API calls\n",
    "max_reply_tokens = 5000\n",
    "temp = 0.2\n",
    "run_date = str(datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")) # Start time of run\n",
    "print(\"Using LLM model: \" + model)\n",
    "\n",
    "# Results Folders\n",
    "raw_results_dir  = ROOT_DIR + \"/LLM_Experiments/Diagnosis_And_Group_Size_Easy/Results/Raw_Results/OpenAI/GPT-4o/\"\n",
    "JSON_results_dir = ROOT_DIR + \"/LLM_Experiments/Diagnosis_And_Group_Size_Easy/Results/Inner_JSON/OpenAI/GPT-4o/\"\n",
    "\n",
    "for paper in paper_list:\n",
    "    # Setup\n",
    "    fn = input_articles_folder + \"/\" + paper\n",
    "    with open(fn, \"r\") as f:\n",
    "        text = f.read()\n",
    "    submit_prompt = base_prompt + text\n",
    "    ml = start_message_list(submit_prompt)\n",
    "    print(\"Starting: \", paper, num_tokens_from_string(str(ml)), \" approx tokens.\")\n",
    "    # LLM Call\n",
    "    if LLM_run:\n",
    "        client = OpenAI()\n",
    "        completion = client.chat.completions.create(\n",
    "            model = model,\n",
    "            response_format = { \"type\": \"json_object\" },\n",
    "            max_tokens = max_reply_tokens,\n",
    "            temperature = temp,\n",
    "            messages = ml\n",
    "        )\n",
    "    else:\n",
    "        print(\"LLM deactivated.\")  # Crash out of loop if LLM deactivated\n",
    "        break\n",
    "    # Save Results\n",
    "    response   = completion.choices[0].message.content # JSON response from LLM\n",
    "    # Filename stuff:\n",
    "    paperID = paper.split(\"_full-text.txt\")[0]   # Note this was an error, but the filenames are fine to use!\n",
    "    # Save:\n",
    "    with(open(JSON_results_dir + \"/\" + paperID + \"_\" + run_date + \".json\", \"w\")) as f:\n",
    "        f.write(response)\n",
    "    with(open(raw_results_dir + \"/\" + paperID + \"_\" + run_date + \".json\", \"w\")) as f:\n",
    "        f.write(completion.model_dump_json(indent=2))\n",
    "    print(\"### Finished: \", paper)\n",
    "\n",
    "print(\"Full run completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
