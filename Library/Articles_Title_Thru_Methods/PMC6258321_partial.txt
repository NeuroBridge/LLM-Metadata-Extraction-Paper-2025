Title.
A Method to Compare the Discriminatory Power of Data-driven Methods: Application to ICA and IVA.
Abstract.
Background.
The widespread application of data-driven factorization-based methods, such as independent component analysis (ICA), to functional magnetic resonance imaging data facilitates the study of neural function and how it is disrupted by psychiatric disorders such as schizophrenia. While the increasing number of these methods motivates a comparison of their relative performance, such a comparison is difficult to perform on real fMRI data, since the ground truth is, relatively, unknown and the alignment of factors across different methods is impractical and imprecise.
New Method.
We present a novel method, global difference maps (GDMs), to compare the results of different fMRI analysis techniques on real fMRI data, quantify their relative performances, and highlight the differences between the decompositions visually.
Comparison with Existing Methods.
We apply this method to compare the performances of two different factorization-based methods, ICA and its multiset extension independent vector analysis (IVA), for the analysis of fMRI data from 109 patients with schizophrenia and 138 healthy controls during the performance of three tasks.
Results.
Through this application of GDMs, we find that IVA can determine regions that are more discriminatory between patients and controls than ICA, though IVA is less effective at emphasizing regions found in only a subset of the tasks.
Conclusions.
These results demonstrate that GDMs are an effective way to compare the performances of different factorization-based methods as well as regression-based analyses.
Introduction.
Due to its high spatial resolution and non-invasive nature, functional magnetic resonance imaging (fMRI) data has become one of the most popular means of understanding normal neural function as well as how it is disrupted by disorders, such as schizophrenia. The data processing strategies for fMRI data can be roughly grouped into two schemes: hypothesis-driven and data-driven. Hypothesis-driven methods exploit actual or supposed a priori knowledge about brain activity and, generally, study neurological relationships across a few regions or with respect to specific stimuli. Data-driven methods, on the other hand, offer a less targeted and more holistic approach, often by decomposing the observed data into a set of factors. Such techniques include: principal component analysis (PCA), independent component analysis (ICA), dictionary learning/sparse coding (DL), non-negative matrix factorization (NMF), tensor-based methods, and independent vector analysis (IVA). The performance of each of these factor models depends on the validity of their modeling assumptions for the dataset being analyzed and, thus, motivates a comparison of different factor models on the same dataset. However, it is difficult to compare the performance of different factor models on real data, since the ground truth is not known and each method typically produces multiple factors.
In order to avoid this issue, many papers that compare different factorization techniques focus on their performance on simulated data, see e.g.,. However, these artificial datasets are usually simple when compared with real fMRI data. When comparing the performance of different factor methods on real fMRI data, most papers align factors from different methods and then rely on a visual comparison, see e.g.,. However, aligning even a subset of the total number of factors from multiple techniques can be time consuming, due to the potentially large number of factors from each method. Additionally, each method exploits different properties of the signal and such comparisons are inherently subjective, since they rely on visual interpretation. Another metric for the comparison of different factorization methods is reproducibility or generalizability, i.e., their ability to produce similar factors across different subjects and sessions. However, focusing solely on reproducibility ignores how informative the extracted factors are for a given task. In the case where multiple groups, such as those affected by a psychiatric disease and those who are healthy are analyzed, the ability of a factor to differentiate between the groups can be used to determine the performance of different factorization methods, see e.g.,. The use of this criterion as a measure of performance is well motivated and exploits the knowledge that there should be some brain-related differences between the groups. However, the technique does not solve the fundamental problem of alignment, since often there are multiple discriminatory factors for each method.
In this paper, we present a novel model comparison technique, global difference maps (GDMs), and demonstrate how they can be used to visually highlight the differences between factorization methods and quantify the discriminative or relational power of a dataset within a decomposition. We apply this technique to highlight the differences between individual analyses, using ICA, and a joint analysis, using IVA, of three fMRI tasks: an auditory oddball (AOD) task, a Sternberg item recognition paradigm (SIRP) task, and a sensorimotor (SM) task. Through this application, we show how GDMs can be an effective way to compare the performances of different factorization-based methods. Results show that IVA can determine regions that are more discriminatory between patients and controls than ICA, however, this improvement in discriminatory power comes at the cost of not emphasizing some of the regions found using ICA in a subset of the tasks.
Materials and Methods.
Feature Extraction.
Since the timing of the stimuli in each task is different, it is difficult to jointly analyze multi-task fMRI data. Rather, for each subject, a simple linear regression is run on the data from each voxel using the statistical parametric mapping toolbox (SPM), where the regressors are created by convolving the hemodynamic response function (HRF) in SPM with the desired predictors for each task. The resulting regression coefficient maps are used as features for each subject and task. This reduction, using a lower-dimensional though still multivariate representation of the data, facilitates exploration of the associations across the features from multiple tasks, see e.g.,, as well as enabling the identification of intrinsic functional networks.
FMRI Tasks and Features.
The datasets used in this study are from the Mind Research Network Clinical Imaging Consortium Collection (publicly available at http://coins.mrn.org). These datasets were obtained from 247 subjects, 109 patients with schizophrenia and 138 healthy controls. Next, we introduce the tasks as well as the associated multivariate features analyzed in this study.
Auditory Oddball Task.
This auditory task required the subjects to listen to three different types of auditory stimuli in a pseudo-random order: standard (1 kHz tones occurring with a probability of 0.82), novel (complex sounds occurring with a probability of 0.09), and target (1.2 kHz tones occurring with a probability of 0.09, to which a right thumb button press was required). Each run of the task consisted of 90 stimuli, each with a 200 ms duration and a randomly changing interstimulus interval of between 550 and 2,050 ms. The order of novel and target stimuli was changed between runs, thus ensuring that the responses were not dependent on the stimulus order. For this task, the regressor was created by modeling both the target and standard stimuli as delta functions and convolving this sequence of delta functions with the default SPM HRF in addition to their temporal derivatives. Subject averaged contrast images between the target versus the standard tones were used as the feature for this task.
Sternberg Item Recognition Paradigm Task.
This visual task required the subjects to remember a set of 1, 3, or 5 randomly chosen numbers between 0 and 9. The task paradigm consisted of: a 1.5 second learn condition, a blank screen for 0.5 seconds, a 6 second encode condition, where the whole sequence of digits was presented together, and a 38 second probe condition, where the subject was shown a sequence of integers and had to indicate, with a right thumb button press, whether or not it was a member of the memorized set. Each probe digit was presented for up to 1.1 seconds in a pseudo-random fashion within a 2.7 second interval. A total of 84 probes, consisting of 42 targets and 42 foils was obtained per scan and the prompt-encode-probe conditions were run twice for each set size in a pseudo-random order. For this task, the regressor was created by convolving the probe response for the three digit set with the default SPM HRF. This was done for both runs of the probe response and then the average map was computed and used as the feature for this task.
Sensory Motor Task.
This auditory task required the subjects to listen to a sequence of auditory stimuli that consisted of 16 different tones each lasting 200 ms and ranging in frequency from 236 Hz to 1,318 Hz with a 500 ms inter-stimulus interval. The first tone presented was at the lowest pitch and each subsequent tone was higher in pitch than the previous one until the highest tone was reached, then the order of the tones was reversed. Each tonal change required a right thumb button press. A run consisted of 15 increase-and-decrease blocks, alternated with 15 fixation blocks, with each block lasting 16 seconds in duration. For this task, the regressor was created by convolving the entire increase- and-decrease block with the default SPM HRF. This was done for both runs for each subject and then the average map was used as the feature for this task.
ICA.
For an fMRI feature dataset from M subjects, , where the mth row of X is formed by flattening the feature of V voxels from the mth subject, the noiseless ICA model can be written as  where the C spatially independent latent sources or neural activation patterns, , are linearly mixed by the mixing matrix, . Given this model, ICA seeks to determine a demixing matrix, W, such that the estimated components, the rows of , are as independent as possible. Since we seek to maximize independence between the estimated components, , estimation of the demixing matrix can be accomplished through the minimization of the mutual information among the components, written as  where H(·) is the entropy.
Since fMRI data is of high dimensionality and quite noisy, i.e., is in a space consisting of signal as well as noise, dimension-reduction using PCA is a crucial preprocessing step in order to avoid over-fitting in subsequent analyses. Determining the appropriate order—the size of the signal subspace—for this PCA step using real fMRI data is an active area of research, see e.g,. Despite its fairly extensive study, the majority of order estimation techniques are appropriate for single subject or single task analyses, which limits their applicability to the joint analysis of multiple tasks, multiset fusion. The reason for this is that these techniques ignore the complementary information between the datasets, which is the main motivation for performing a joint analysis on such data. In this work, we use the order estimated by a modification of the procedure in. The reason for this is because, to the best of our knowledge, it is the only method that has shown desirable performance for the task of order estimation in both the sample-poor and sample-rich regimes inherent to the fusion of medical imaging data. The technique, named PCA and canonical correlation analysis (PCA-CCA), estimates the size of the signal subspace shared by two datasets, i.e., the common order, through a sequence of binary hypothesis tests. The process begins by assuming that the common order is 0 and that the null hypothesis is that the current common order is appropriate. If the null hypothesis is rejected, the common order is increased by 1 and the test is repeated, until the null hypothesis cannot be rejected or the common dimension is equal to half the number of subjects, M/2. In this application, we estimate the order for each pairwise combination of tasks and then select the highest estimated order to enable the retention of the most complementary information across the datasets.
We should note that although there are many ICA algorithms, in this work, we used the ICA by entropy bound minimization (EBM) algorithm, due to the fact that it has shown superior performance in both simulated and real neurological data when compared with the popular Infomax algorithm, see e.g.,. This improved performance derives from the fact EBM does not assume a fixed form for the distribution of the latent sources and instead attempts to upper bound their entropy through the use of measuring functions. The use of these measuring functions enables the description of a wide variety of distributions, including those that are: unimodal, bimodal, symmetric, and skewed, thus, generally, improving the estimation of all sources within the mixture.
IVA.
FMRI data from subjects performing multiple tasks has been increasingly gathered during the same study, since each task is expected to provide related information regarding neural function and how it is impacted by neurological diseases, such as schizophrenia, see e.g.,. In order to address this scenario, we extend the model in (1) to K datasets, or tasks, as  Due to the scaling and permutation ambiguities inherent to ICA, running a separate ICA individually for each task and aligning the results is both impractical and suboptimal, since it would ignore the complementary information that each task provides. This motivates the performance of a joint analysis, such as using IVA, a recent multiset extension of ICA, which exploits similarities across datasets to achieve a successful decomposition.
The mutual information cost function for IVA can be written as,  where  is the cth source component matrix (SCM), formed by concatenating the cth estimated component from each of the datasets. Note that the difference between (2) and (4) is that we are now minimizing the mutual information between SCMs and not components. This cost function reduces to the sum of separate ICAs performed on each dataset individually if the sources in each of the datasets are mutually independent of each other. In this work, IVA via a multivariate Gaussian and Laplacian algorithm is used, since it has proven to be an effective IVA algorithm for analysis of fMRI data, see e.g.,.
GDMs.
As discussed in Section 2.3, since the data from each subject within a task has been reduced to a feature, the columns of the estimated mixing matrix, A, provide the weight of each component across the subjects. This means that the ith column of the estimated mixing matrix, ai, represents the subject-specific weights of the ith source estimate, si. Thus, if the goal of the study is to determine a set of explanatory factors that can be used to assess differences between two groups, as is the primary focus for this work where the groups are patients with schizophrenia and healthy controls, the weights can be used to determine if any component is expressed differently across the two groups. In order to statistically determine components that have different expressions, on average, between patients and controls, a two-sample t-test is performed on columns of A, where one group is represented by the weights corresponding to the patients with schizophrenia and the other by the weights corresponding to the healthy controls. The formula for the two-sample t-test is presented below  where μi, , ni are the mean, variance, and number of samples in group i. Note that when the difference between the weights of the two groups is statistically significant, the component describes functional differences between patients and controls and is referred to as a possible biomarker of disease. The t-statistics derived from these tests can serve as the basis for the construction of the GDMs.
Before describing how we construct GDMs, we should note that, since an important focus of this study is differentiating patients with schizophrenia from healthy controls, we focus on how GDMs can highlight the brain regions that differentiate between two groups. However, as we describe at the end of this section, GDMs can also be constructed such that they reflect the regions that are correlated with certain behavioral variables of interest, highlighting associations between psychological tests and neural function.
Due to the scaling ambiguity of ICA, prior to the construction of GDMs, Z-scores are independently computed from the individual biomarkers using the mean and standard deviation computed using all voxels. Let N be subset of the C components estimated using separate ICAs or IVA for each dataset with weights that are statistically significantly different, at p < 0.05, between patients and controls. Denoting these components as , we construct the GDM for that method and dataset, , as follows  where tn is the t-statistic, calculated as described in Section 2.3, for the nth component that is positive or made to be positive by multiplying the corresponding subject covariation and component by −1. Note that due to the sign ambiguity associated with ICA and IVA, multiplying both the component as well as the subject covariation by −1 will produce the same solution. The physical interpretation of such a transformation is that, for example, rather than having patients have increased activation in a certain region over controls, controls have increased deactivation over patients. The GDM can be seen as a summary map that describes only the regions that activate/deactivate significantly differently between patients and controls for a given decomposition and dataset within that decomposition. Each biomarker is scaled by the value of its corresponding t-statistic, so has more weight if the component is better able to differentiate between patients and controls. Note that though we describe the construction of GDMs for factors extracted using ICA and IVA, they could also be constructed using the results of any factor analysis, such as PCA, NMF, or DL. Additionally, though beyond the scope of this work, GDMs can also be defined based upon regression models, which are useful when there are less clearly defined groups. Though we should note that using general linear models where the regressor is the disease state of the subjects is, generally, less robust than factor models in this case.
We can quantify the discriminative power of a GDM, and thus, indirectly, the whole decomposition, by generating component weights in a nearly identical manner to the GDM spatial maps and performing a two-sample t-test on the resulting weights. The corresponding component weights for the GDM are given by  where  is the subject covariation, obtained using ICA or IVA, corresponding to  and, just as for , is multiplied by −1 if the sign of tn is negative. This construction is expected to result in component loadings that show greater discriminatory power than the original , since  is constructed from only those component loadings that are statistically significant. Additionally, the  can be modeled as unit step functions with weights corresponding to their disease status and corrupted with Gaussian noise, where higher values of tn imply lower variances for the noise. Therefore, the summation of multiple , weighted as in (7), will tend to result in a lower value for the variance of the noise and, therefore, a more discriminative subject covariation. Additionally, note that the t-tests performed on the columns of the mixing matrices may be seen as a feature extraction step, thus corrections for multiple comparisons should only account for the number of GDMs calculated.
We should note that, by construction, the intensity of each voxel in a GDM is dependent on a linear combination of voxel values across components. For this reason, the GDMs produced by algorithms that have fewer overlapping voxels across components are expected to be more easily interpretable. This is true for methods that promote sparsity, such as: Infomax, IVA-L, or dictionary learning. This is also true for methods that make few assumptions about the distribution of the latent sources, such as: EBM or SparseICA-EBM, in addition to methods that account for sample dependence between voxels, such as: weights-adjusted second-order blind identification as well as ICA and IVA using entropy-rate bound minimization.
A fundamental goal of neuroimaging studies is to understand the underlying neural basis for psychological measurements. GDMs lend themselves to this goal in a straightforward manner and through their application can facilitate the discovery of which neuroimaging datasets most effectively explain the results of psychometric tests. This extension can be done by using the t-statistics derived from the correlations of the subject weights with the behavioral variables of interest, rather than those derived using the two-sample t-test. This means that the functional form of (6) and (7) would not change, instead tn would be defined based on the sample correlation and  would be selected based on the significance of their correlation to the test score of interest. In this study, the behavioral variables were derived from the measurement and treatment research to improve cognition in schizophrenia (MATRICS) consensus cognitive battery (MCCB), which is widely recognized as a valuable tool that provides a comprehensive evaluation of cognitive function of schizophrenia within clinical trials.
Simulation Setup.
For real brain data, information about the true latent sources and subject weights is, generally, not known apriori. Therefore, in order to demonstrate the efficacy of GDMs, we generate 5 simulated sources, shown in Figure 1. We then linearly mix the simulated latent sources with a simulated mixing matrix whose columns have a step-type characteristic corrupted by additive i.i.d. Gaussian noise, simulating a latent difference between two groups of subjects, each with 150 subjects in order to make the scenario similar to the real fMRI data used in this study. Thus, the equation for the columns of the mixing matrix is given by  where ni is i.i.d. white Gaussian noise, generated separately for each column, and ci is the step profile of size 1 for c1, 0.5 for c2, 0.26 for c3, 0.23 for c4, and 0 for c5. The differing values of the step height correspond to different average relative differences between the groups and range from high to no difference. Note that since we know the value of the mean difference between the groups as well as the standard deviation of the samples (subjects), we are able to determine theoretical values for the t-statistics for each column of A. We then reduce the dimension of the dataset from 300 to 5 using PCA, perform ICA, and estimate a GDM.
The purpose of this simulation is twofold. First, we seek to demonstrate the utility of GDMs to visually summarize the results of a decomposition. Second, to show how it is possible to use GDMs to relate the performance of different factorization methods. Towards this second aim, in this simulation, we compare the performance of two ICA methods, EBM and FastICA using the hyperbolic tangent nonlinearity. We selected FastICA primarily because it is a popular method for the analysis of fMRI data, see e.g.. However, FastICA uses a fixed nonlinearity and so it may not able to fit a very wide variety of distributions meaning that the performance of FastICA may suffer when the distribution of the latent sources is different from the one implied by its nonlinearity. In contrast, as already mentioned in Section 2.3, EBM is able to fit a wide variety of distributions. We present the results of this analysis in Figure 1. Note that in order to test the consistency of the results, those presented here are the result of 1,000 independent runs.