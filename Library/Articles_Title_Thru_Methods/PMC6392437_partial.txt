Title.
The Role of Diversity in Data-driven Analyses of Multi-subject fMRI Data.
Abstract.
Data-driven methods have been widely used in functional magnetic resonance imaging (fMRI) data analysis. They extract latent factors, generally, through the use of a simple generative model. Independent component analysis (ICA) and dictionary learning (DL) are two popular data-driven methods that are based on two different forms of diversity—statistical properties of the data—statistical independence for ICA and sparsity for DL. Despite their popularity, the comparative advantage of emphasizing one property over another in the decomposition of fMRI data is not well understood. Such a comparison is made harder due to the differences in the modeling assumptions between ICA and DL, as well as within different ICA algorithms where different algorithms exploit different forms of diversity. In this paper, we propose the use of objective global measures, such as time course frequency power ratio, network connection summary and graph-theoretical metrics, to gain insight into the role that different types of diversity have on the analyses of fMRI data. Four ICA algorithms that account for different types of diversity and one DL algorithm are studied. We apply these algorithms to real fMRI data collected from patients with schizophrenia and healthy controls. Our results suggest that no one particular method has the best performance using all metrics, implying that the optimal method will change depending on the goal of the analysis. However, we note that there is no scenario where the popular Infomax has the best performance, demonstrating the cost to of exploiting fewer forms of diversity.
Introduction.
In blind source separation problems, such as functional magnetic resonance imaging (fMRI) data analysis, it is beneficial to summarize the observed data through a latent factor model. However, little is known about the processes underlying the generation of the factors. This motivates the development of data-driven methods, which extract latent factors generally through the use of a simple generative model. Data-driven methods have proven useful for the analysis of fMRI data –. Different forms of diversity—types of statistical properties—of the underlying sources, such as independence and sparsity, are adopted to achieve the decomposition, thus resulting in the development of different data-driven techniques.
One such technique that has proven quite popular is independent component analysis (ICA) that takes (statistical) independence among the underlying sources into account,,. This popularity has, in part, led to the development of different ICA algorithms which are derived from the maximum likelihood principle, each designed to achieve the independent component decomposition through exploiting different forms of diversity of the signals, such as higher-order statistics, noncircularity, and sample dependence –. For example, the Infomax algorithm, which was the first algorithm used for fMRI analysis, exploits higher-order statistics through the use of a fixed tangent hyperbolic non-linearity. In contrast to Infomax, entropy bound minimization (EBM) and entropy rate bound minimization (ERBM) are two more recently introduced ICA algorithms and have been shown to provide desirable performance on both simulated and real fMRI data, see e.g.,, –, through the use of a dynamic nonlinearity, and in the case of ERBM, sample dependence as well. The use of a dynamic nonlinearity enables EBM and ERBM to match a wide variety of distributions.
Besides independence, sparsity, another form of diversity that has been exploited in many fields, has proven to be a useful property in the decomposition of fMRI signals –. An increasingly popular sparsity exploiting method that has become popular for the analysis of fMRI data is dictionary learning (DL),,. Its popularity stems, in part, from the fact that it uses only sparsity to determine functional networks of interest, and that it does not require the modeling of a source distribution. Instead, it aims at balancing the decomposition accuracy and source sparsity through a regularization parameter.
Due to the desirable performance of methods that take either sparsity or independence into account, a unified mathematical framework that enables dynamic exploitation of both independence and sparsity has been proposed, and it is referred to as sparse ICA framework. Source sparsity is incorporated through the use of the ICA cost function, penalized by an ℓ1 regularization term.
Although independence and sparsity have demonstrated their utility for the analysis of fMRI data, there is no study that explores the role of each diversity in terms of global metrics for real fMRI datasets. Those few studies that have investigated the performance of Infomax, EBM and ERBM, used a limited number of subjects and based the evaluation on subjective metrics, such as visual inspection of a few well-matched components,. Additionally, there has been no comparison of the performance of these methods with sparse ICA framework and DL. Such an exploration raises the issue of how to determine a metric for comparing the performance of different data-driven algorithms on real fMRI data without a ground truth. Algorithmic comparison is difficult as decompositions can be quite different depending on the modeling assumptions of the particular algorithm, thus matching of all the estimated components is usually not possible. This motivates the use of more objective global metrics for algorithmic performance.
The purpose of this work is to gain insight into the role of different types of diversity on a decomposition. For this, Infomax, EBM, ERBM, DL and sparse ICA framework are studied. Additionally, the relative performance of the different algorithms is assessed on a large real fMRI dataset consisting of 179 subjects. Due to the increasing number of large fMRI datasets that include hundreds or even thousands of subjects, understanding the performance of these techniques in this scenario increases our confidence in the generalizability of the results. In order to be fair to each algorithm, we propose to use objective global measures, e.g., time course frequency power ratio,, network connection summary, and graph-theoretical metrics,. Time course frequency power ratio indicates whether a component is describing the blood-oxygenation-level-dependent (BOLD) response in fMRI data or not. Network connection summary gives a general idea on how well each algorithm can reconstruct the complex connections in brain. Graph-theoretical metrics are an efficient tool for studying the heterogeneity between different groups of subjects, such as patients with schizophrenia (SZs) and healthy controls (HCs) –. These metrics perform a global comparison of the algorithms based on all the components or the whole brain functional connectivity.
Through this comparison, we find that the use of global metrics for a performance comparison can provide a general guide to the practitioners about the selection of the appropriate algorithm for a specific situation. For instance, DL produces components comprising signal that is more likely to be derived from the BOLD response, EBM yields better clustering within functional networks, and ERBM is better in capturing group differences and yields higher variance in SZs than HCs when using graph-theoretical metrics.
The rest of the paper is organized as follows. In Section II, we introduce the data that is used, describe the group ICA framework as well as the four ICA algorithms, DL, and describe the three global measures in detail. In Section III, we present the experimental results. We discuss the results in Section IV and conclude with Section V.
Materials and methods.
Data acquisition.
The data used in this study is a resting state fMRI data from the Center of Biomedical Research Excellence (COBRE), which is available on the collaborative informatics and neuroimaging suite data exchange repository (http://coins.mrn.org/dx),. The data includes 88 SZs (average age: 37 ± 14) and 91 HCs (average age: 38 ± 12). All images were collected on a single 3-Tesla Siemens Trio scanner with a 12-channel radio frequency coil using the following parameters: TE = 29 ms, TR = 2 s, flip angle = 75°, slice thickness = 3.5 mm, slice gap = 1.05 mm, voxel size 3.75 × 3.75 × 4.55 mm3. Participants were instructed to keep their eyes open during the scan and stare passively at a central fixation cross. Each resting state scan consists of 150 volumes. To eliminate the T1-related signal fluctuations (T1 effect), the first 6 volumes are removed in this study, thus 144 volumes remain for each subject. The fMRI data are realigned with INRIalign algorithm, slice-timing correction is applied using the middle slice as the reference frame in the functional data pipeline and spatially normalized to the standard Montreal Neurological Institute space and resampled to 3 × 3 × 3 mm3, resulting in 53 × 63 × 46 voxels. Afterwards, the fMRI data are smoothed using a Gaussian kernel with a full-width at half-maximum of 10 mm.
Group ICA.
The group ICA framework enables analysis of fMRI data from multiple subjects. Let the observed fMRI data from the kth subject be denoted by , 1 ≤ k ≤ K, where T denotes the number of time points and V denotes the number of voxels. To reduce the contamination from noise, principal component analysis (PCA), using an order suggested by the entropy rate based order selection technique described in, is employed to reduce the dimension of the data for each subject. The order estimation method proposed in takes sample dependence into consideration without downsampling, which leads to improved estimation of the signal subspace. For each subject, the dimension of  is reduced from T to T′, computed as , where † represents the pseudoinverse and  is the subject level reduction matrix, whose columns are the eigenvectors of  and the reduced data is . The reduced data matrix, X[k], consists of the first T′ principal components of  that represent the informative signals from the kth subject. It is assumed that the subjects share a common component subspace. In order to estimate the common signals across subjects, the datasets are temporally concatenated to form a single data matrix , which is then reduced to  by a group level PCA, , with  as the group level reduction matrix and N as the order for the common observation subspace. Group components  are then estimated by performing ICA on the common group subspace Y,  where  is the mixing matrix. ICA seeks to find a group demixing matrix, W, such that the estimated sources are obtained as . The use of a single ICA on the common subspace of all datasets helps to preserve the order of the components across subjects. Following the completion of ICA, back-reconstruction is performed on  to generate the corresponding subject-specific source estimates . In order to obtain the back-reconstructed signals, , the group level reduction matrix G† is blocked by columns, G† = [(G[1])†, (G[2])†, ⋯, (G[K])†] with  and,  is reconstructed using , and the corresponding subject-specific time courses are obtained using .
Independence-based ICA: Algorithm choice.
The differences in separation performance for separate ICA algorithms, such as Infomax, EBM and ERBM, are related to differences in their assumed latent source models. In order to estimate the demixing matrix W, Infomax and EBM equivalently aim at minimizing the mutual information between the source estimates . The corresponding cost function is  where H(·) = −E[logp(·)] refers to (differential) entropy of a random variable, with p referring to the probability density function (PDF) of corresponding variable. The last term H(Y) is a constant as it is independent of W. Estimation of the PDF of sources is a crucial task. To resolve this task, Infomax takes only the diversity of higher-order statistics into consideration using a distribution model implied by a fixed sigmoidal nonlinearity. This fixed nonlinearity is a good match for very focal regions of activation, however it might significantly bias latent sources relating to broad regions, such as the default mode network (DMN). In contrast to Infomax, EBM does not assume one specific distribution for the latent sources but instead attempts to upper bound their entropy through the use of several measuring functions. Each of these functions provides bounds on the entropy, with the tightest bound being closest to the true entropy. The use of these measuring functions makes it possible to match a wide variety of distributions, including those that are sub-Gaussian, super-Gaussian, unimodal, bimodal, symmetric, as well as skewed, thus potentially leading to more accurate estimation of the latent sources.
Instead of bounding the entropy of latent sources, ERBM attempts to bound their entropy rate using measuring functions. The cost function, thus, is given by  where  refers to the entropy rate of , thus the cost function becomes the mutual information rate between the source estimates. Note that the third term Hr (Y) is a constant and wont be counted in optimization. By calculating the entropy rate of sources, one more type of diversity, namely, sample dependence, is taken into consideration. Since EBM and ERBM relax the assumptions placed on the fMRI sources by assuming flexible source distributions, they are expected to provide improved performance over Infomax. Additionally, ERBM is expected to have superior performance over EBM as well as Infomax, since it takes advantage of the underlying properties of the fMRI components, namely, higher-order statistics and voxel-wise dependence.
Sparsity-based DL.
DL is one of the data-driven method that only takes the sparsity of latent sources into consideration. Recently, it has been successfully applied to fMRI data analysis,. DL expresses the observations as sparse combinations of the atoms (columns) in a dictionary , seeking to estimate the latent sources  that is conveyed in the sparse loadings. The cost of DL procedure is given by  where  are the spatial maps, , and λ is the regularization parameter. The DL algorithm used in this work achieves the decomposition using an online optimization algorithm, making it suitable for large datasets with millions of samples.
Balancing independence and sparsity.
Due to the desirable performance of methods that take either sparsity or independence into account, recently, a unified mathematical framework is proposed to account for both independence and sparsity of sources by incorporating a sparsity term into the cost function of ICA,. EBM is utilized to demonstrate its application and the corresponding algorithm is referred to as SparseICA-EBM. The cost function of SparseICA-EBM is constructed of two terms, i.e., an independence term and a sparsity term. The independence refers to (2) and the sparsity is written as the summation of the regularization function of all estimates. The final cost function is given by   where  is the regularization term with respect to the nth source estimate , λn is the sparsity parameter and ϵn is the smoothing parameter. The third term in (2) is removed here since it is a constant with respect to W. From the cost function we can see that different sparsity and smoothing parameters enable different estimates.
Parameter selection.
One important parameter in the application of an ICA algorithm is the model order, i.e., the number of common signals, N. However, for fMRI data, classical order estimation techniques based on information theoretic criteria may overestimate the order due to the inherent sample dependence of fMRI data,. A common way to overcome this issue is by using downsampling to obtain effectively independent and identically distributed samples,. Unfortunately, methods based on downsampling suffer from a loss of information associated with the downsampling. More recently, two entropy rate (ER)-based order estimation techniques are proposed that account for sample dependence without the use of downsampling: ER using a finite memory length model (ER-FM) and ER using an autoregressive model (ER-AR). Therefore, these two methods are used in this paper to estimate the number of common signals.
As introduced above, SparseICA-EBM seeks to achieve the decomposition by assuming both independence and sparsity using a sparsity parameter λn, which enables the sparse solution for the nth source, . The balance between independence and sparsity can be adjusted by tuning λn. Smaller λn in (5) emphasizes independence more, and larger λn places greater weight on sparsity. Another parameter for SparseICA-EBM is the smoothing parameter ϵn. In (5), the regularization term of sparsity was originally , which is non-differentiable. To resolve this issue, it is replaced by the sum of multi-quadratic functions, as given by (6). Higher smoothing parameter ϵn will help to produce smoother sources. However, the effect of this parameter with application to fMRI data analysis has not been explored. For this reason, in this work, different values of λn and ϵn are considered.
Similarly, an appropriate value should be set for the regularization parameter λ for DL. The analytic link between λ and the corresponding effective sparsity of  is not clearly investigated. The performance of DL is highly dependent on the selection of λ for the tradeoff between accuracy and sparsity. There has been no clear guidance for the choice of λ in real world applications. Hence, in this work, we investigate the influence of different values of λ on the performance for our relatively large real fMRI data. There is another key parameter for DL in practice, the number of atoms in dictionary. As we are seeking to compare the performance of ICA algorithms and DL algorithm, to make it easier for implementation, we set the atom number to be the same with the number of latent sources that is estimated for ICA.
Global metrics for performance evaluation.
To compare the performance of these five algorithms, proper measures are needed. Since it can be difficult to exactly match all of the estimated components across different algorithms, it is hard to directly compare algorithmic performance on real fMRI data. In order to resolve this issue, we propose the use of global measures to compare the performances of data-driven algorithms on real data. Following are three global measures that are used.
Frequency analysis:.
The first global measure of data-driven algorithmic performance on real fMRI data is the ratio of time course power spectra in low-frequency band (< 0.1Hz) to the high-frequency band (> 0.15 Hz) for each IC. Since the activation in the components is due to the BOLD response, which corresponds to the low frequencies, the higher power ratios implies that the components are more closely associated with true neural function. Conversely, the lower the ratio, the more likely the component is to be describing cardiac or respiratory noise as opposed to true BOLD activation,.
Network connection summary:.
Another global measure of the performance is network connection summary. Prior to the construction of network connection summary, for each algorithm, M ICs out of N that refer to brain functional networks are selected based on their time course power ratio and visual inspection. First, all the components are separated into two categories, one containing those with time course power ratio higher than 3, and the other containing the remaining ones. After that, through visual inspection the components with large edge effects and ventricles are removed from the first category, and those with low power ratio but obviously refer to meaningful network are put back to the first category, which is finally used for network connection analysis. Using time course power ratio to separate components into two categories in the first step will significantly reduce the difficulty of visual inspection in the second step.
For ICA algorithms, though they assume that the latent sources are fully independent, following the performance of ICA the extracted ICs generally have some dependence due to their functional relevance. Consequently, after identifying the functional networks that are conveyed in the ICs, we estimate the brain connectivity by calculating the full-order statistical dependence, mutual information (MI), among them. The functional connectivity is constructed using normalized MI, , as the measure. The normalized MI is given by  where  is the MI between two estimated components  and . The performance of three Shannon entropy based MI estimation methods, k-nearest neighborhood, analytical formula corresponding in the chosen exponential family and Parzen window based method is investigated. The former two methods are from information theoretical estimators (ITE) toolbox (https://bitbucket.org/szzoli/ite/). Our exploration shows that for super-Gaussian variable, the method proposed in performs the best, and it is used in our work. After the normalization, the connectivity between two similar sources would be close to 1 and that of two dissimilar sources would be close to 0.
Functional networks identified from these data-driven algorithms are expected to have the ability to reconstruct the brain network connections. A better connection reconstruction shows better decomposition performance and will improve the results of post analyses, such as clustering, on these estimated networks. To measure the modularity of the functional networks, the ratio of the average intra-module connectivity to the average inter-module connectivity is defined as:  where Q is the number of modules, Ni is the number of connections within the ith module, Ni,j is the number of connections between the ith and jth modules, eu and ev refer to the intra- and inter-module connectivity respectively, and Nintra and Ninter refer to the total number of intra- and intermodule connectivity, respectively. The larger the ratio, the more compact the modules are.
Graph-theoretical metrics:.
Graph-theoretical metrics are other global measures that are used for performance comparison. For each algorithm and subject, the M selected components are used to construct the graph. With the M ICs of interest as nodes and the normalized MI between them as the edges, e, a fully connected graph, G, is constructed. Beginning with G, a edge threshold, et, is used to retain only the highest P percent of the edges, thus generating a new graph G. We define the percentage of the edges that remain after thresholding as the link density, which increases as the threshold decreases. The weighted graphs are converted to binary ones, with the edges below the threshold et having a value of 0 and those above having a value of 1. In order to avoid very sparse graphs with small link densities and those with too large link densities, we limit the link density to range from 20% to 70%.
Graph-theoretical analysis is done on these graphs by calculating different graph metrics. There have been several reviews, see,,, showing that graph metrics highlight different topological characteristics of graphs. The characteristic path length (PL), global efficiency and centrality are measures that can facilitate functional integration in graphs. They are globally calculated, which means that all the other nodes are taken into consideration in their calculation, and provide measures of how information is transferred in the functional network. Clustering coefficient (CC) is a measure that can capture the segregation of networks by measuring the transfer of information in the immediate neighborhood of each node. The global graph CC that is averaged across all the nodes is explored. Small-worldness (SW) of the network is also calculated to measure the degree of small-world organization in the overall functional network. Table. I presents the metrics that are used in this work. The formulas of these metrics are described in detail in –. All the implementations are performed using Matlab codes in the Brain Connectivity Toolbox (https://sites.google.com/site/bctnet/).